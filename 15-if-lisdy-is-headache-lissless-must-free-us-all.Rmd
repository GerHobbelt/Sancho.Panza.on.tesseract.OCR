well, not really.
the compression image artifacts are gone, yes they are, but I never said it were image compression artifacts *specifically* which caused those OCR errors and produced those adversarial input images! let's see what good old sensor noise can do!

holy mackerel! the OCR engine is speaking in tongues while this page clearly is utterly empty! how home?

well, first guess is always betting on stupidity. stupidity of mammals of the ape persuasion, to e more specific: homo sapiens rather not so sapiens.

take, for example, input image assumptions. yes, the majority of us snigger at the worn-out joke of "assuming makes an ass out of you and me", but do we really comprehend the impact, the corrolary if this position? do we truly *grok* what we do?
well, plenty times I dont or I do but plod on regardless, and I've seen plenty two legged donkeys during my tour of liff, so here goes one prime example:
while designing/architectung/coding/testing  your input image preprocessing pipeline, it is most reasonable to assume said image will contain some cintent to OCR, right? after all, why would anyone be interested in expending the notable effort of running an OCR chain for processing an empty image? that'd be nuts, right? right!

until you have a book scanned, front to cover, automatically or otherwise rugirously, so you gave it digitized front to back. then, it turns out, there's the front a back cover and maybe the publisher got fancy on your ass because it's top quality, stitched hardcover and that means your page count is a folio multiple whatever your author wrote, because saddle stitched hardcover means you can't have odd oage count (likewise fir glued paperback, so duh! tell me something new!) not can you have any page count that is *not* divisible by four. unless your publisher is one of the very rare remaining old skool experts and is still able to sneak in a shortened leaf (ie one side of the page sheet in the bundle clipped to a small depth!). anyhoo, when you take this baby and feed it to the OCR engine, photographed page by page, you sure as shit are going yo encounter one if more pages left entirely, embarrassingly, naked. a blank! no "this page intentionally left blank" left page types to accommodate book user experience designers who dictate every major chapter must start at right, ie an odd page, for best reader experience. nothing if the sort! a page, butt naked in the nuge!

meanwhile your smart programmer you did assume every input page carrying some cintent which was translated to "the page will show a bimidal distribution in its histogram" and action is taken accordingly, in both the image contrast enhancing stage, if any, and your threshold er algo. oops, hello tesseract! didn't see you enter the room, sorry old pal, all's well and swell, I bet?

not really, not any more, no. this page is giving me the clap and I took several minutes to chew it so here's my entry into the "can a monkey with a banana and a typewriting produce poetry at the level if James Joyce?" I think I got pretty darn close this time, or so my delusional confidence rankings tell me. whaddayathink?

content less pages can also be noticeably bimodal. sure. either thanks to the luscious natural laws surrounding quantised noise (which is what a clean, pixelated empty page scan is, arguably) which tell us that the supposedly monomodal histogram distribution may exhibit additional random (sub)peaks due to the image noise, which does not bow to make thetask college freshmen math level easy but will spit out rather spiky, noisy!, histograms instead, which will happily accomodate any search-the-valley-in-the-bimidal-dist by providing valleys small and large galore.

oh, you say, there's smoothing for that. so we smooth the histogram. okay, plenty nudes and empty heads will now be easier to detect and preselect for the alternate duty called "flagging as empty page".
current tesseract mainline doesn't do this (yet?) but it's a start, it's a start.
still you do realize this is a finicky stopgap measure as I have here one very much nonempty scan, okay, it's contrast is extremely mediocre, but you do realize that, when you crank that smoothing parameter up to near max, thus baby will be your next Waterloo as it's now indistinguishable from a clean empty page?

ok, so we keep it civil with the smoothing strength setting. then we gave this here empty page, which has a bit of a gradient alas, thanks to uneven page illumination during the photography session, which is something all of us has had to suffer a zillion times, even when using highest grade scanning tables engineered for photographing/digitising those extremely delicate historical volumes at the Met and elsewhere us musea of high sophistication: the gradient together with the camera sensor noise is surely sufficient to randomly produce multimodal image histograms, even after the strictest of histogram smoothing regimes, so you are in an arms race here, you see?
we cannot afford to press those aged leaves down in various spots just for you to get a histogram matching your earlier naive assumption! (let's use that, yes. naive. so sweet, so young, far removed from jaded curmydgeonious me. beautiful.)

so we need more image preprocessing stages, more work done before we can have a gander at a page histogram to see if we should send the bugger up Nude And Empty Lane or allow it to proceed through the main thoroughfare towards OCR nirvana and total bliss.

regrettably, real life has a tendency to side with curmydgeonious old farts like me after a while, probably because real life gets fed up with happy fluffy unicorn mantras after a short while or possibly because real life is just a positively nasty bugger that might have stepped  out if the containment grid rigged up in the old testament, where some allegedly monotheistic gid smites, kills and universally obliterates anything that might serve as a trigger for his daily tantrums, while having a hissy fit over his main manservant trying on a little independence for a bit; fallen angel indeed. no review board, no hearing, just kicked in the nuts and thrown in the cellar without anything approaching due coarse. yep, sounds to me that author grokked the essence of real life: always a nasty surprise lurking in the next dark alley, flashing the knife that'll gut our poor pedestrian stepping along the OCR sidewalk towards, what exactly?

yeah, image processing is a lot if drama and, as in life, it's the little touches, the finessing, that makes the grade. this time around.

TLDR / the moral? reckon with the need for adjustable / configurable / scriptable inagf preprocessing piepelines of arbitrary, possibly large, depth and complexity and reckon that you need to keep this configuration ) script / setting with every image you grab, fir some will not be born equal,ie: some if your input inages will require (mini batch?) customized prepricessing adjystments to pass the mark. no matter what you try or do, there will always be some, few or many, that do t adhere to your world view and will need a tweaked pipeline. design accordingly: facilitate this rather that attempting to prevent this: that is futile, ultimately 

