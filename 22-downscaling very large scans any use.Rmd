is it any use?

tesseract has a built-in limitation on the maximum image width & height: the TDimension type is int16_t and the maximum width and height are specified as 32767 pixels wide.

of course, such extremely large images do take a lot if time, so it might be beneficial to reduce these in size before feeding them to the OCR engine: while there does exist a patch for tesseract to accept higher dimension imagery, I did ultimately not enable that patch in my repo as the image processing cost is ludicrously high.

# thought: how does same content yet different image size impact tesseract performance? some O() expression perhaps, or a chart plotting the increasing cost: is this linear or squared per image dimension factor?

when downscaling images, the question arises which algo works best for OCR: while some people may like Lanczos et al for human perceptually good downscales (how do those single pixel lines and curves come out, though?), one can seriously wonder whether that sort of approach is best for OCR .

might it not, perhaps, be best to use a custom downscaler that keeps the thin lines in the foreground i.e. the text on the page intact as best as possible, say a max filter over nearest neighbour? think Bodoni title font and then downscaling that to xheight 30px 



which brings us to the added relevance of this downscaling question: when tesseract were able to segment detect dropcaps or title lines, those must be downscales for they won't otherwise fit the 50px vertical scanline nature vector size of the LSTM engine, so even for smaller input images, when you have dropcaps and I've title lines or other relatively large text lines (presentation sheet scans, f.e.?) tesseract will do some downscaling of its own for these parts of the image, after it's done segmenting.


